{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df01502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 保存训练集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595e9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/CVT/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde36875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "340000"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = df.loc[:, ['a', 'b', 'c']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "numpy_train = df.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numpy_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../data/CVT/cvt_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(numpy_train, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 测试集数据处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704de53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/CVT/test.csv\")\n",
    "\n",
    "# 假设我们有一个标签列表，初始时全是 0，表示正常数据\n",
    "labels = np.zeros(len(df))\n",
    "\n",
    "\n",
    "# 设置异常数据的长度和数量\n",
    "num_anomalies = 200  # 需要插入异常数据的数量\n",
    "anomaly_length = 50  # 每个异常区段的长度（例如50行）\n",
    "\n",
    "# 生成异常数据的起始位置\n",
    "np.random.seed(2024)  # 设置随机种子以确保结果可重现\n",
    "anomaly_start_indices = np.random.choice(len(df) - anomaly_length, num_anomalies, replace=False)\n",
    "\n",
    "# 对每个异常区段进行处理\n",
    "for start_idx in anomaly_start_indices:\n",
    "    # 在这段连续区间内加入异常，假设我们在第0列数据上增加0.3%模拟异常\n",
    "    df.iloc[start_idx:start_idx + anomaly_length, 0] *= 1.005  # 这里模拟在第0列增加1%的异常\n",
    "    labels[start_idx:start_idx + anomaly_length] = 1  # 将这段区间标记为异常\n",
    "\n",
    "# 将标签添加为 DataFrame 的一列\n",
    "df['Label'] = labels\n",
    "\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "print(df['Label'].sum())\n",
    "df_label = df.iloc[:,-1]\n",
    "df_test = df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "numpy_test = df_test.to_numpy()\n",
    "numpy_label = df_label.to_numpy()\n",
    "\n",
    "with open(\"../data/CVT/cvt_test_05.pkl\", \"wb\") as f:\n",
    "    pickle.dump(numpy_test, f)\n",
    "\n",
    "with open(\"../data/CVT/cvt_test_label_05.pkl\", \"wb\") as f:\n",
    "    pickle.dump(numpy_label, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#  第二种方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# 假设有一个原始数据框\n",
    "np.random.seed(40)  # 设置随机种子以确保可复现\n",
    "df = pd.read_csv(\"../data/CVT/test.csv\")\n",
    "\n",
    "n = df.shape[0]\n",
    "\n",
    "# 随机选择多个误差时间段\n",
    "num_error_segments = np.random.randint(200, 210)  # 随机选择误差段数，200到250段\n",
    "\n",
    "# 用于标记误差的列\n",
    "df['label'] = 0\n",
    "\n",
    "# 存储已使用的误差段区间\n",
    "occupied_intervals = []\n",
    "\n",
    "# 设置每列发生误差的概率\n",
    "error_probabilities = {'a': 0.8, 'b': 0.5, 'c': 0.3}  # 'a' 列 80% 概率，'b' 列 50% 概率，'c' 列 30% 概率\n",
    "\n",
    "# 生成误差段\n",
    "error_segments = []\n",
    "for _ in range(num_error_segments):\n",
    "    # 随机选择误差段的开始位置和持续时间\n",
    "    error_start = np.random.randint(0, n - 100)  # 错误开始位置\n",
    "    error_length = np.random.randint(50, 60)  # 错误持续时间\n",
    "\n",
    "    # 记录这个误差段\n",
    "    occupied_intervals.append((error_start, error_length))\n",
    "\n",
    "    # 根据设置的概率选择需要施加误差的列\n",
    "    columns_to_error = []\n",
    "    for col, prob in error_probabilities.items():\n",
    "        if np.random.rand() < prob:  # 根据概率决定是否施加误差\n",
    "            columns_to_error.append(col)\n",
    "\n",
    "    # 随机选择误差幅度，在 0.2% 到 0.3% 之间浮动\n",
    "    error_percentage = np.random.uniform(0.002, 0.003)\n",
    "\n",
    "    # 增加单向异常的概率：有一定概率只对一方向施加误差\n",
    "    if np.random.rand() < 0.7:  # 70% 的概率施加单向异常\n",
    "        error_direction = np.random.choice([1, -1])  # 正向或反向的异常\n",
    "        error_percentage *= error_direction\n",
    "    else:\n",
    "        # 双向误差的处理，误差在 [0.2%, 0.3%] 之间浮动\n",
    "        error_percentage *= np.random.choice([1, -1])\n",
    "\n",
    "    # 记录误差段\n",
    "    error_segments.append((error_start, error_length, columns_to_error, error_percentage))\n",
    "\n",
    "# 对每个误差段施加误差\n",
    "for error_start, error_length, columns_to_error, error_percentage in error_segments:\n",
    "    for col in columns_to_error:\n",
    "        df.loc[error_start:error_start + error_length, [col]] *= (1 + error_percentage)\n",
    "    df.loc[error_start:error_start + error_length, 'label'] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df_label = df.iloc[:,-1]\n",
    "df_test = df.iloc[:,:-1]\n",
    "\n",
    "numpy_test = df_test.to_numpy()\n",
    "numpy_label = df_label.to_numpy()\n",
    "\n",
    "with open(\"../data/CVT/cvt_test_03.pkl\", \"wb\") as f:\n",
    "    pickle.dump(numpy_test, f)\n",
    "\n",
    "with open(\"../data/CVT/cvt_test_label_03.pkl\", \"wb\") as f:\n",
    "    pickle.dump(numpy_label, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
